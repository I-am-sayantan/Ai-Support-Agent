# AI Support Agent ğŸ¤–

**Intelligent RAG-powered support agent with Azure OpenAI**

An AI agent that answers questions using either direct LLM knowledge or retrieves relevant information from your documents using semantic search (RAG). Exposed via a clean REST API built with FastAPI.

---

## ğŸ“‹ Table of Contents

- [Architecture](#-architecture)
- [Features](#-features)
- [Quick Start](#-quick-start)
- [API Usage](#-api-usage)
- [Project Structure](#-project-structure)
- [Testing](#-testing)

---

## ğŸ—ï¸ Architecture

### Simple Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            CLIENT                                   â”‚
â”‚                   (Web, Mobile, curl, etc.)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    POST /ask {query, session_id}
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FASTAPI SERVER (api.py)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Receive Request                                            â”‚  â”‚
â”‚  â”‚ 2. Get/Create Session â†’ Each session has its own RAG Agent    â”‚  â”‚
â”‚  â”‚ 3. Pass query to Agent                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG AGENT (api.py)                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Process Query with Conversation History                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                             â–¼                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚  Azure OpenAI GPT-4o-mini    â”‚                      â”‚
â”‚              â”‚  (Function Calling Enabled)  â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                             â”‚                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚                             â”‚                       â”‚
â”‚              â–¼                             â–¼                       â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚      â”‚   General   â”‚              â”‚  Company Info â”‚                â”‚
â”‚      â”‚   Question  â”‚              â”‚    Question   â”‚                â”‚
â”‚      â”‚             â”‚              â”‚               â”‚                â”‚
â”‚      â”‚ "What is    â”‚              â”‚ "How many     â”‚                â”‚
â”‚      â”‚  Python?"   â”‚              â”‚  remote days?"â”‚                â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚             â”‚                             â”‚                        â”‚
â”‚             â”‚                             â”‚ Call Tool:             â”‚
â”‚             â”‚                             â”‚ search_documents()     â”‚
â”‚             â”‚                             â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
              â”‚                             â–¼
              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚   RAG SYSTEM (rag_system.py)         â”‚
              â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
              â”‚              â”‚  â”‚ 1. Convert query â†’ embedding   â”‚  â”‚
              â”‚              â”‚  â”‚    (text-embedding-3-large)    â”‚  â”‚
              â”‚              â”‚  â”‚                                â”‚  â”‚
              â”‚              â”‚  â”‚ 2. Search FAISS Vector Store   â”‚  â”‚
              â”‚              â”‚  â”‚    (3072-dim, L2 similarity)   â”‚  â”‚
              â”‚              â”‚  â”‚                                â”‚  â”‚
              â”‚              â”‚  â”‚ 3. Return top 3 chunks         â”‚  â”‚
              â”‚              â”‚  â”‚    with source files           â”‚  â”‚
              â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
              â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
              â”‚                             â”‚ Document chunks
              â”‚                             â”‚ as context
              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚                             â”‚
              â”‚              â–¼                             â”‚
              â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
              â”‚       â”‚   Azure OpenAI GPT-4o   â”‚          â”‚
              â”‚       â”‚   Generate answer with  â”‚          â”‚
              â”‚       â”‚   retrieved context     â”‚          â”‚
              â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
              â”‚                     â”‚                      â”‚
              â–¼                     â–¼                      â”‚
        Direct Answer         Answer + Sources             â”‚
              â”‚                     â”‚                      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                         â”‚                                 â”‚
                         â–¼                                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
              â”‚  Response to API   â”‚                       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                         â”‚                                 â”‚
                         â–¼                                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
              â”‚ {                              â”‚           â”‚
              â”‚   "answer": "...",             â”‚           â”‚
              â”‚   "source": "llm|documents",   â”‚           â”‚
              â”‚   "session_id": "..."          â”‚           â”‚
              â”‚ }                              â”‚           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
                                                           â”‚
DATA STORAGE:                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   documents/     â”‚  â”‚   rag_index/    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ - policies.txt   â”‚  â”‚ - index.faiss   â”‚  Built from docs
â”‚ - handbook.txt   â”‚  â”‚ - chunks.pkl    â”‚  13 chunks total
â”‚ - faq.txt        â”‚  â”‚ - metadata.pkl  â”‚
â”‚ - security.txt   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ - technical.txt  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points:**

- **2 Paths**: Direct LLM answer OR Document search via RAG
- **Smart Routing**: AI automatically decides which path to use
- **Session Memory**: Each session maintains its own conversation history
- **Source Tracking**: Know if answer came from LLM knowledge or documents

---



### How It Works

1. **Client** sends a question via REST API (`POST /ask`)
2. **FastAPI** router creates/retrieves session and initializes RAG Agent
3. **RAG Agent** uses Azure OpenAI's function calling to intelligently decide:
   - Answer directly from LLM knowledge (general queries)
   - OR invoke document search tool (company-specific queries)
4. **RAG System** (if document search triggered):
   - Converts query to 3072-dimensional embedding
   - Searches FAISS vector store using L2 similarity
   - Returns top 3 most relevant document chunks with scores
5. **Azure OpenAI** generates final answer using retrieved context
6. **Response** returned as JSON with answer, source attribution, and session ID

---

## âœ¨ Features

- âœ… **Semantic Search**: FAISS vector database with 3072-dim embeddings
- âœ… **Smart Routing**: AI decides when to search documents vs answer directly
- âœ… **Session Memory**: Maintains conversation context per session
- âœ… **REST API**: FastAPI with automatic OpenAPI docs
- âœ… **Document Chunking**: Intelligent 500-word chunks with overlap
- âœ… **Source Attribution**: Tracks which documents were used

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set Up Environment

Create a `.env` file with your Azure OpenAI credentials:

```env
AZURE_OPENAI_API_KEY=your-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT=your-chat-deployment-name
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=your-embedding-deployment-name
AZURE_OPENAI_API_VERSION=2024-12-01-preview
```

### 3. Build RAG Index

```bash
python rag_system.py
```

This will:

- Load documents from `documents/` folder
- Split them into chunks
- Generate embeddings
- Build FAISS index
- Save to `rag_index/` folder

### 4. Start the API Server

```bash
uvicorn api:app --reload
```

Server will start at `http://127.0.0.1:8000`

---

## ğŸ“¡ API Usage

### Interactive API Docs

Open in your browser:

- **Swagger UI**: http://127.0.0.1:8000/docs
- **ReDoc**: http://127.0.0.1:8000/redoc

### Ask a Question

```bash
curl -X POST "http://127.0.0.1:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is our remote work policy?",
    "session_id": "user123"
  }'
```

**Response:**

```json
{
  "answer": "Our remote work policy allows employees to work remotely up to 3 days per week...",
  "source": "documents",
  "session_id": "user123"
}
```

### All Available Endpoints

- `GET /` - Health check
- `POST /ask` - Ask a question
- `GET /sessions` - List all active sessions
- `DELETE /sessions/{session_id}` - Clear a session
- `POST /rebuild-index` - Rebuild RAG index
- `GET /health` - API health status

---

## ğŸ“ Project Structure

```
Ai-Support-Agent/
â”‚
â”œâ”€â”€ api.py                    # FastAPI server + RAG Agent
â”œâ”€â”€ rag_system.py            # RAG implementation (chunking, embeddings, FAISS)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md               # This file
â”‚
â”œâ”€â”€ .env                    # Azure OpenAI credentials (create this)
â”‚
â”œâ”€â”€ documents/              # Knowledge base documents
â”‚   â”œâ”€â”€ company_policies.txt
â”‚   â”œâ”€â”€ employee_handbook.txt
â”‚   â”œâ”€â”€ product_faq.txt
â”‚   â”œâ”€â”€ security_policy.txt
â”‚   â””â”€â”€ technical_docs.txt
â”‚
â””â”€â”€ rag_index/             # FAISS vector store (auto-generated)
    â”œâ”€â”€ index.faiss
    â”œâ”€â”€ chunks.pkl
    â””â”€â”€ metadata.pkl
```

### File Descriptions

- **`api.py`**: Complete FastAPI application with RAG-powered agent, session management, and all endpoints
- **`rag_system.py`**: RAG pipeline - document chunking, embedding generation, FAISS vector store, semantic search
- **`documents/`**: Your knowledge base - add more `.txt` files here and rebuild the index
- **`rag_index/`**: Persisted FAISS index - automatically created when you run `rag_system.py`

---

## ğŸ§ª Testing

### Test with Python

```python
import requests

response = requests.post(
    "http://127.0.0.1:8000/ask",
    json={
        "query": "How many days of annual leave?",
        "session_id": "test"
    }
)
print(response.json())
```

### Test with curl

```bash
# General question (uses LLM knowledge)
curl -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Python?", "session_id": "test"}'

# Document question (uses RAG)
curl -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "What is our security policy?", "session_id": "test"}'

# Follow-up (uses session memory)
curl -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "Tell me more about that", "session_id": "test"}'
```

---

## ğŸ“š Adding New Documents

1. Add `.txt` files to the `documents/` folder
2. Rebuild the index:
   ```bash
   python rag_system.py
   ```
   OR call the API:
   ```bash
   curl -X POST http://127.0.0.1:8000/rebuild-index
   ```
3. New documents are now searchable!

---

## ğŸ”§ Configuration

Edit `.env` to change:

- Azure OpenAI model/deployment
- API version
- Endpoint URL

Edit `rag_system.py` to adjust:

- Chunk size (default: 500 words)
- Chunk overlap (default: 50 words)
- Top-k results (default: 3)

Edit `api.py` to modify:

- System prompts
- API endpoints
- Session management

---

## ï¿½ What Was Built - Task Summary

### Task 1: AI Agent Development âœ…

**Objective**: Build an AI agent with tool calling capabilities

**What Was Implemented**:

- âœ… **Azure OpenAI Integration**: Connected to gpt-4o-mini model for chat completions
- âœ… **Tool Calling System**: Implemented function calling with `search_documents()` tool
- âœ… **Prompt Engineering**: Created system prompts that instruct the AI to:
  - Answer general questions directly using LLM knowledge
  - Automatically invoke document search for company-specific queries
  - Decide intelligently when to use which approach
- âœ… **Session Memory**: Built conversation history tracking that:
  - Maintains context across multiple queries
  - Persists per session ID
  - Enables follow-up questions
- âœ… **Agent Architecture**: Created `RAGAgent` class in [api.py](api.py) with:
  - `process_query()` method for handling user input
  - `search_documents()` method for document retrieval
  - Automatic tool execution and response generation

**Technical Details**:

- Model: `gpt-4o-mini` (Azure OpenAI)
- Tool: Function calling with JSON schema
- Memory: List-based conversation history with role-based messages

---

### Task 2: RAG Implementation âœ…

**Objective**: Implement Retrieval-Augmented Generation with document embeddings

**What Was Implemented**:

- âœ… **5 Sample Documents Created**:

  1. `company_policies.txt` - Remote work policy, leave policies, code of conduct
  2. `employee_handbook.txt` - Benefits, compensation, working hours
  3. `product_faq.txt` - CloudSync Pro product information, pricing, features
  4. `security_policy.txt` - Encryption standards, access control, compliance
  5. `technical_docs.txt` - API documentation, authentication, rate limits

- âœ… **Document Chunking System** ([rag_system.py](rag_system.py)):

  - `DocumentChunker` class splits documents into 500-word chunks
  - 50-word overlap between chunks for context preservation
  - Maintains metadata (source filename) for each chunk

- âœ… **Embedding Generation**:

  - Model: `text-embedding-3-large` (Azure OpenAI)
  - Vector dimensions: 3072-dimensional embeddings
  - Auto-detection of embedding dimensions
  - Batch processing for efficiency

- âœ… **FAISS Vector Store**:

  - `FAISSVectorStore` class for similarity search
  - L2 distance metric (IndexFlatL2)
  - Stores 13 total chunks from 5 documents
  - Persistence to disk in `rag_index/` folder

- âœ… **Semantic Search**:
  - `RAGSystem` orchestrates the entire pipeline
  - Query â†’ Embedding â†’ FAISS Search â†’ Top-K results
  - Returns top 3 most relevant chunks with scores
  - Source attribution for each result

**Technical Details**:

- Embeddings: `text-embedding-3-large` (3072-dim)
- Vector DB: FAISS with L2 similarity
- Chunks: 13 chunks, 500 words each, 50-word overlap
- Index saved to: `rag_index/index.faiss`, `chunks.pkl`, `metadata.pkl`

---

### Task 3: Backend API âœ…

**Objective**: Create REST API using FastAPI

**What Was Implemented**:

- âœ… **FastAPI Application** ([api.py](api.py)):

  - Production-ready ASGI server with Uvicorn
  - CORS middleware for cross-origin requests
  - Auto-generated OpenAPI documentation at `/docs`
  - Pydantic models for request/response validation

- âœ… **6 API Endpoints**:

  1. `GET /` - Root endpoint with API info
  2. `POST /ask` - Main endpoint for asking questions
     - Input: `{query: str, session_id?: str}`
     - Output: `{answer: str, source: str, session_id: str}`
  3. `GET /health` - Health check with RAG index status
  4. `GET /sessions` - List all active sessions
  5. `DELETE /sessions/{session_id}` - Clear specific session
  6. `POST /rebuild-index` - Rebuild RAG index from documents

- âœ… **Session Management**:

  - In-memory session storage (dict-based)
  - Each session has its own `RAGAgent` instance
  - Maintains conversation history per session
  - Auto-generates UUID if no session_id provided

- âœ… **RAG Integration**:
  - Agent uses tool calling to decide: LLM vs Document search
  - Semantic search via FAISS when documents needed
  - Source tracking (returns "llm" or "documents")
  - Context-aware responses using conversation history

**Technical Details**:

- Framework: FastAPI with Uvicorn
- Port: 8000 (default)
- Session Storage: In-memory dictionary
- Response Format: JSON with answer, source, session_id
- Documentation: Auto-generated Swagger UI at `/docs`

---
